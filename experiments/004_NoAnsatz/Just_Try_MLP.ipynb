{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKOGauT-PwG2"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "9mwPy-axPzv4",
    "outputId": "6de0a395-0313-486e-9036-b4fca3de6bde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclarkmiyamoto\u001b[0m (\u001b[33mclarkmiyamoto-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vSSrlPvfJRbu"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17e6NfjzI4wR"
   },
   "source": [
    "## Deform Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bWrpo37TI5m8"
   },
   "outputs": [],
   "source": [
    "def sin_distortion(x_length: int,\n",
    "                   y_length: int,\n",
    "                   A_nm: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Sin distortion for creating deformation maps.\n",
    "\n",
    "    Args:\n",
    "    - x_length (int): Length of x-axis of image.\n",
    "    - y_length (int): Length of y-axis of image.\n",
    "    - A_nm (torch.Tensor): Square matrix of coefficients. Sets size of cut off.\n",
    "\n",
    "    Returns:\n",
    "    (torch.Tensor, torch.Tensor): Deformation maps for x and y coordinates.\n",
    "    \"\"\"\n",
    "    if A_nm.shape[0] != A_nm.shape[1]:\n",
    "        raise ValueError('A_nm must be square matrix.')\n",
    "\n",
    "    A_nm = A_nm.float()\n",
    "\n",
    "    # Create Coordinates\n",
    "    x = torch.linspace(-1, 1, x_length, dtype=torch.float32)\n",
    "    y = torch.linspace(-1, 1, y_length, dtype=torch.float32)\n",
    "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "    # Create Diffeo\n",
    "    x_pert = torch.linspace(0, 1, x_length, dtype=torch.float32)\n",
    "    y_pert = torch.linspace(0, 1, y_length, dtype=torch.float32)\n",
    "\n",
    "    n = torch.arange(1, A_nm.shape[0] + 1, dtype=torch.float32)\n",
    "    x_basis = torch.sin(torch.pi * torch.outer(n, x_pert)).T\n",
    "    y_basis = torch.sin(torch.pi * torch.outer(n, y_pert))\n",
    "\n",
    "    perturbation = torch.matmul(x_basis, torch.matmul(A_nm, y_basis))\n",
    "\n",
    "    x_map = X + perturbation\n",
    "    y_map = Y + perturbation\n",
    "\n",
    "    return x_map, y_map\n",
    "\n",
    "def apply_transformation(image_tensor,\n",
    "                         A_nm: torch.Tensor,\n",
    "                         interpolation_type='bilinear'):\n",
    "    \"\"\"\n",
    "    Wrapper of `sin_distortion`. Gets torch.tensor and returns the distorted\n",
    "    torch.tensor according to A_nm.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): Inputted image.\n",
    "        A_nm (torch.Tensor): Characterizes diffeo according to `sin_distortion`.\n",
    "        interpolation_type (str): Interpolation method ('bilinear' or 'nearest').\n",
    "\n",
    "    Returns:\n",
    "        image_tensor_deformed (torch.Tensor): Diffeo applied to `image_tensor`.\n",
    "    \"\"\"\n",
    "    # Create deformation map\n",
    "    x_length, y_length = image_tensor.shape[1:3]\n",
    "    x_map, y_map  = sin_distortion(x_length, y_length, A_nm)\n",
    "\n",
    "    return apply_flowgrid(image_tensor, x_map, y_map, interpolation_type=interpolation_type)\n",
    "\n",
    "\n",
    "def apply_flowgrid(image_tensor, x_map, y_map, interpolation_type='bilinear'):\n",
    "    # Stack and unsqueeze to form grid\n",
    "    grid = torch.stack((y_map, x_map), dim=-1).unsqueeze(0).to(image_tensor.device)\n",
    "\n",
    "    # Apply grid sample\n",
    "    image_tensor_deformed = torch.nn.functional.grid_sample(image_tensor.unsqueeze(0),\n",
    "                                                            grid,\n",
    "                                                            mode=interpolation_type,\n",
    "                                                            align_corners=True)\n",
    "\n",
    "    return image_tensor_deformed.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "WBzlbca7I_bl",
    "outputId": "89cb5cbf-dd14-44b7-eb7b-4f244603a7e6"
   },
   "outputs": [],
   "source": [
    "def diffeo_dataset(tensor):\n",
    "    A_nm = torch.tensor([[0.0, 0.14],\n",
    "                         [-0.02, 0.01]])\n",
    "    return apply_transformation(tensor, A_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzPEH1H1IzEj"
   },
   "source": [
    "# Get ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDJTqfNJIwDm",
    "outputId": "e6f7789e-8f63-46dd-a533-e7e226e5f897"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "root = '/imagenet/'\n",
    "total_images = 1000\n",
    "pct_train = 0.8\n",
    "\n",
    "\n",
    "num_train = int(total_images * pct_train)\n",
    "num_val = total_images - num_train\n",
    "\n",
    "# Preprocess the image w/o diffeo\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "])\n",
    "\n",
    "# Preprocess the image w/ diffeo\n",
    "preprocess_diffeo = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    diffeo_dataset,\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dataset_train_images_og = torchvision.datasets.ImageNet(root=root, \n",
    "                                                        split='train', \n",
    "                                                        transform=preprocess, )\n",
    "dataset_train_images_diffeo = torchvision.datasets.ImageNet(root=root, \n",
    "                                                            split='train', \n",
    "                                                            transform=preprocess_diffeo,)\n",
    "dataset_val_images_og = torchvision.datasets.ImageNet(root=root, \n",
    "                                                      split='val', \n",
    "                                                      transform=preprocess,)\n",
    "dataset_val_images_diffeo = torchvision.datasets.ImageNet(root=root, \n",
    "                                                     split='val', \n",
    "                                                     transform=preprocess_diffeo)\n",
    "\n",
    "dataset_train_images_og = Subset(dataset_train_images_og, indices=range(num_train))\n",
    "dataset_train_images_diffeo = Subset(dataset_train_images_diffeo, indices=range(num_train))\n",
    "dataset_val_images_og = Subset(dataset_val_images_og, indices=range(num_val))\n",
    "dataset_val_images_diffeo = Subset(dataset_val_images_diffeo, indices=range(num_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scAfZZW3Io3X"
   },
   "source": [
    "# Image Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbU_-xFyIIdp",
    "outputId": "f7674bc9-ef3d-44ad-a0ca-49602bcd1c8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ViT model\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:04<00:00, 179.80it/s]\n",
      "100%|██████████| 800/800 [00:05<00:00, 149.98it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 141.80it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 121.68it/s]\n"
     ]
    }
   ],
   "source": [
    "tensor_train_images_og     = torch.cat([dataset_train_images_og[i][0].unsqueeze(0).to(device) for i in tqdm(range(len(dataset_train_images_og)))])\n",
    "tensor_train_images_diffeo = torch.cat([dataset_train_images_diffeo[i][0].unsqueeze(0).to(device) for i in tqdm(range(len(dataset_train_images_og)))])\n",
    "tensor_val_images_og       = torch.cat([dataset_val_images_og[i][0].unsqueeze(0).to(device) for i in tqdm(range(len(dataset_val_images_og)))])\n",
    "tensor_val_images_diffeo   = torch.cat([dataset_val_images_diffeo[i][0].unsqueeze(0).to(device) for i in tqdm(range(len(dataset_val_images_og)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C2Ekk5d-I2KE"
   },
   "outputs": [],
   "source": [
    "def get_activation(model, input, layer_index: list):\n",
    "\n",
    "  activation = {}\n",
    "  def getActivation(name):\n",
    "      # the hook signature\n",
    "      def hook(model, input, output):\n",
    "          activation[name] = output.detach()\n",
    "      return hook\n",
    "\n",
    "  handles = []\n",
    "  def retrieve_layer_activation(model, input, layer_index):\n",
    "    if len(input) == 3: input = input[None, :, :, :]\n",
    "\n",
    "    layers = list(model.children())\n",
    "    layers_flat = flatten(layers)\n",
    "\n",
    "    for index in layer_index:\n",
    "      handles.append(layers_flat[index - 1].register_forward_hook(getActivation(str(index))))\n",
    "\n",
    "    with t.no_grad(): model(input)\n",
    "    for handle in handles: handle.remove()\n",
    "\n",
    "    return\n",
    "\n",
    "  def flatten(array):\n",
    "      result = []\n",
    "      for element in array:\n",
    "          if hasattr(element, \"__iter__\"):\n",
    "              result.extend(flatten(element))\n",
    "          else:\n",
    "              result.append(element)\n",
    "      return result\n",
    "\n",
    "  retrieve_layer_activation(model, input, layer_index)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6w49aA67XxwP",
    "outputId": "511e3f52-2cb7-422d-ca04-4523c92ab217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Parameters 512\n",
      "torch.Size([800, 512])\n",
      "torch.Size([800, 512])\n"
     ]
    }
   ],
   "source": [
    "layer_id = 13\n",
    "\n",
    "activation_train_og = get_activation(model, tensor_train_images_og, [layer_id])[f'{layer_id}'].flatten(start_dim=1).to('cpu')\n",
    "activation_train_diffeo = get_activation(model, tensor_train_images_diffeo, [layer_id])[f'{layer_id}'].flatten(start_dim=1).to('cpu')\n",
    "activation_val_og = get_activation(model, tensor_val_images_og, [layer_id])[f'{layer_id}'].flatten(start_dim=1).to('cpu')\n",
    "activation_val_diffeo = get_activation(model, tensor_val_images_diffeo, [layer_id])[f'{layer_id}'].flatten(start_dim=1).to('cpu')\n",
    "\n",
    "# Dataset to attempt representation finding\n",
    "train_dataset = TensorDataset(activation_train_og, activation_train_diffeo)\n",
    "val_dataset = TensorDataset(activation_val_og, activation_val_diffeo)\n",
    "\n",
    "print('Num Parameters', activation_train_og.shape[-1])\n",
    "print(activation_train_og.shape)\n",
    "print(activation_train_diffeo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t35KQWuBPYk5"
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0UhptqU2PXzw"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    # Model\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrCs8MVS09h"
   },
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oqoW2TBR54e",
    "outputId": "7f0a42bc-c042-4312-df87-7a670c532219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2036224\n"
     ]
    }
   ],
   "source": [
    "model_ginv = Decoder()\n",
    "model_ginv.to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_ginv.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 1. Initialize Weights & Biases\n",
    "# -------------------\n",
    "wandb.init(\n",
    "    project=\"diffeo\",\n",
    "    name=f\"MLP\",\n",
    "    config={\n",
    "        \"epochs\": 1500,\n",
    "        \"batch_size\": 1,\n",
    "        \"lr\": 1e-5,\n",
    "        \"weight_decay\":0.01,\n",
    "        \"dataset\": \"ImageNet1k\",\n",
    "    },\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "beg1NhuaT-3v",
    "outputId": "7a834c14-82b3-490b-9778-118bed064e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline tensor(0.2336)\n",
      "Epoch [1/1500] | Train Loss: 1.2882 | Val Loss: 0.5017\n",
      "Epoch [2/1500] | Train Loss: 0.4628 | Val Loss: 0.4952\n",
      "Epoch [3/1500] | Train Loss: 0.4606 | Val Loss: 0.5062\n",
      "Epoch [4/1500] | Train Loss: 0.4602 | Val Loss: 0.4968\n",
      "Epoch [5/1500] | Train Loss: 0.4599 | Val Loss: 0.4941\n",
      "Epoch [6/1500] | Train Loss: 0.4582 | Val Loss: 0.4974\n",
      "Epoch [7/1500] | Train Loss: 0.4577 | Val Loss: 0.4917\n",
      "Epoch [8/1500] | Train Loss: 0.4570 | Val Loss: 0.4940\n",
      "Epoch [9/1500] | Train Loss: 0.4567 | Val Loss: 0.4904\n",
      "Epoch [10/1500] | Train Loss: 0.4557 | Val Loss: 0.4944\n",
      "Epoch [11/1500] | Train Loss: 0.4563 | Val Loss: 0.4899\n",
      "Epoch [12/1500] | Train Loss: 0.4555 | Val Loss: 0.4911\n",
      "Epoch [13/1500] | Train Loss: 0.4553 | Val Loss: 0.4908\n",
      "Epoch [14/1500] | Train Loss: 0.4550 | Val Loss: 0.4954\n",
      "Epoch [15/1500] | Train Loss: 0.4546 | Val Loss: 0.4910\n",
      "Epoch [16/1500] | Train Loss: 0.4541 | Val Loss: 0.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14c73458c180>\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/ext3/miniforge3/lib/python3.12/multiprocessing/process.py\", line 142, in join\n",
      "    def join(self, timeout=None):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_Y \u001b[38;5;241m=\u001b[39m batch_Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ginv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, batch_Y)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[40], line 31\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m-> 1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1741\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Define Model, Optimizer, Loss\n",
    "# -------------------\n",
    "\n",
    "optimizer = optim.AdamW(model_ginv.parameters(), \n",
    "                        lr=config.lr, \n",
    "                        weight_decay=config.weight_decay,\n",
    "                      )\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Baseline error on the first sample\n",
    "print('Baseline', criterion(*train_dataset[0]))\n",
    "\n",
    "# -------------------\n",
    "# Training Loop\n",
    "# -------------------\n",
    "for epoch in range(config.epochs):\n",
    "    # ---- Training ----\n",
    "    model_ginv.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        # Move to GPU if available\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model_ginv(batch_X)\n",
    "        loss = criterion(predictions, batch_Y)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Average training loss over the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model_ginv.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_Y = batch_Y.to(device)\n",
    "\n",
    "            predictions = model_ginv(batch_X)\n",
    "            loss = criterion(predictions, batch_Y)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # ---- Logging ----\n",
    "    # Log both training loss and validation loss at the end of each epoch\n",
    "    wandb.log({\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{config.epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVlhFij9X4k2"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset\n",
    "pic_id = 1\n",
    "\n",
    "x, y = train_dataset[pic_id][0], train_dataset[pic_id][1]\n",
    "x_plot = x.flatten().cpu()\n",
    "y_plot = y.flatten().cpu()\n",
    "prediction = model_ginv(x.unsqueeze(0))\n",
    "prediction_plot = prediction.flatten().cpu().detach().numpy()\n",
    "\n",
    "plt.plot(x_plot, label='input', alpha=0.3)\n",
    "plt.plot(y_plot, label='target', alpha=0.3)\n",
    "plt.plot(x_plot - y_plot, label='baseline')\n",
    "plt.plot(y_plot - prediction_plot, label='residual', color='black')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VV-dlcJX_6d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
